import Route from '@ember/routing/route';

export default Route.extend({

    model() {

        return [

            {
                title: "GeoPredict",
                description: "This series details the development of a novel machine learning solution called GeoPredict to leverage mobility data by making next place predictions for shopping trips.",
                posts: [{
                    title: "Part 1: Origins & Data Exploration",
                    snippet: "In this section we start by exploring the data and defining the problem domain.",
                    body: "<p><strong>Introduction&nbsp; </strong></p> <p>This series of blog posts details the development of a novel machine learning solution called GeoPredict. I designed GeoPredict with the intent of leveraging large-scale mobility data in a commercially viable manner. I conducted this effort on behalf of an industry partner while working in the laboratory of the Western Engineering Dean of Research, in my capacity as a Summer Research Associate.</p> <p>The first step towards the development of GeoPredict was gaining a solid understanding of the problem domain-the data. The data provided by my industry partner was not just raw mobility data but a richer set of contextual and semantic information. Let us try to unpack the structure of this data.</p> <p>&nbsp;</p> <p><strong>Understanding the Data</strong></p> <p>We call a set of data a MobilityTrace and a discrete data point a Visit. My partner collected the provided MobilityTrace from their user base (millions of users) over three months.</p><div class='text-center'><img src='/img/blog/GeoPredict/1a.png' class='img-fluid py-5' style='max-width:70%;'></div>&nbsp; <p>A Visit is created when a Source captures a new Coordinate. A Coordinate contains the position of the Source (longitude and latitude) and the precision of this measurement.</p> <p>Precision is the attribute of a Coordinate accounting for the uncertainty in the position calculation. It is the radius of the circle surrounding the given position. Each Source uses different methods for determining position. These include but are not limited to WiFi triangulation, cellular triangulation, and GPS. Each of these methods has different ranges of baseline precision that vary due to environmental factors.</p> <p>A Source itself is the specific platform upon which my partner&rsquo;s application runs. Sources include web browsers, mobile operating systems, and mobile browsers.&nbsp; As people tend to keep their cell phones on their person Visits collected from mobile Sources are vital because they serve as a better proxy for human mobility than static Sources such as a desktop web browser.</p> <p>A userID is an attribute assigned to each Visit to enable analysis on a per-user basis. A user could have multiple ids if they are not logged in across Sources. For privacy, my partner opted not to tie each Visit to a complete UserProfile but only the numeric userID.&nbsp;</p> <p>The most significant preprocessing step on the provided MobilityTrace was tagging each Visit with a commercial POI (point of interest). Each POI is an individual location belonging to a POIGroup. For example, Starbucks is a POIGroup, but the Starbucks with the address 1 Bayside Drive is a POI.</p> <p>POI tagging was accomplished using a commercial database licensed by my partner. My partner removed all Visits that could not be tagged.&nbsp; This purge is significant in that any value to be had from the MobilityTrace would relate to shopping trips as opposed to general mobility.</p> <p><strong>&nbsp;</strong></p> <p><strong>Unsupervised Approach</strong></p> <p>Once I fully understood the nature of the data, I turned my attention to refining my goal: what type of value could my solution deliver? My first intuition was to see if I could uncover any hidden structural value with an unsupervised learning approach. I started with just one feature, using the distance between the Coordinates to cluster the Visits with k-means. Upon inspection of the resulting clusters, I realized that I had stumbled upon a method of discovering POIs. At a low resolution the group labels represented entire geographic regions (cities and towns), but at a higher resolution, the labels signified POIs.</p> <p>By utilizing a third-party database, my partner was already able to label these clusters. I wondered, however, what would happen as new POIs emerged. Would my partner depend on their data provider to provide timely updates? Was there a way I could build a system to discover and label new POIs to break this dependence? I started brainstormed some potential solutions.</p> <p>The first approach that came to mind was to match the cluster coordinates to an address on a city map and then build a web crawler to find mentions of the address on social media with the intention of extracting a POI name. Another potential solution would be to collect photos geotagged with cluster&rsquo;s coordinates and use computer vision techniques to obtain the names of the storefronts in these pictures. I reckoned Google might use a similar approach with the vast amount of Street View imagery.&nbsp; Although both these techniques were of interest to me from an academic standpoint, they were outside my operating parameters in that in that they would require additional data (city maps, user photos) and thus not be commercially viable to my partner.</p> <p>I continued by trying to cluster a higher dimensional set of features considering time, distance and category. I used principal component analysis to produce a unified distance metric for input into k-means. Upon inspection of the results, I was unable to draw any useful conclusions. I decided it was time to move on.</p> <p><strong>&nbsp;</strong></p> <p><strong>Supervised Approach: Recommender Systems</strong></p> <p>Frustrated with my lack of progress I resolved to binge-watch an entire season of Stranger Things on Netflix. Upon completion of the season, I was left impressed. Not just by the wonderfully nostalgic world of Hawkins but with Netflix's suggestion for what I might watch next.&nbsp; Netflix aptly combined the context of Stranger Things with my viewing habits to come up with a strong set of candidates. It was the idea of context that intrigued me. If I liked Stranger Things, then I would probably like Aliens too. Could I use the context of a recent Visit to predict the next one?</p> <p>What makes the ability to predict a user's next Visit valuable? Suppose a user, Alice, is currently at a Goodlife Fitness and we have a recommender system predict that she will go to McDonald's next. Other business in the Food &amp; Beverage space could participate in a real-time bid for the ability to market to Alice right before she makes her decision. The confidence of the recommendation could help the bidders tune their bids.&nbsp; A second potential application is an emergency alert system. Let us say that the police just evacuated the McDonalds Alice was likely to visit; an alert could be sent to Alice warning her to steer clear. The applications of these recommendations go on and on- the value is obvious. Additionally, the system seemed feasible given the MobilityTrace I was working with and my time constraints. Thus it was clear- my goal would be to build a near real-time recommender system for shopping trip forecasting.</p>"
                },
                {
                    title: "Part 2: Mobility Markov Chains (MMCs)",
                    snippet: "Now we explore the Mobility Markov Chain forming the basis of our model.",
                    body: "<p><strong>Introduction</strong></p><p>Having gained a solid grasp of both the problem and solution domains it was time to move towards implementation. How do we achieve effective next place that trip prediction is a subset of the general mobility problem I figured it was unnecessary to reinvent the wheel, so I started researching techniques which consider complete MobilityTraces. After reviewing the literature, I landed on Markov Chains as the basis of my implementation. While there are techniques involving neural networks that are slightly more accurate, they don't pay dividends considering their complexity and required computing power. The drivers of human mobility are not so involved as to need neural layers to capture them.</p><p>&nbsp;</p><p><strong>Mobility Markov Chains</strong></p><p>A Markov Chain is a stochastic model in which the frequency of transition between states governs the probability distribution of the random variable. Many subdisciplines of A.I. use Markov Chains in some way or another, and every application has its domain-specific nomenclature. In the mobility context, we adopt the name Mobility Markov Chain (MMC). MMCs have an intuitive visualization as a state diagram as seen below.</p><div class='text-center'><img src='/img/blog/GeoPredict/2a.png' class='img-fluid py-5' style='max-width:40%;'></div><p>In this MMC the nodes signify state (a specific POI) while the labels of the directed edges represent the probability of transition from one POI to the next. The simplest way to train an MMC is to record the counts of past transitions between two states using a set of training data (MobilityTrace). At prediction time we can build a probability distribution by looking at all the edges connected to the current state.</p><p>The MMC above considers one piece of context, the current POI, to define the state of the system. However, we could consider more context in our predictions and calculate the transition probabilities not just based on a single edge put on a path. Let us define some new terminology to describe the amount of context used in the calculation of the transition probabilities. &nbsp;&nbsp;&nbsp;</p> &nbsp;<p style='text-align: center;'><em> Let an (N-MMC) be an MMC in which we calculate probabilities for paths of length N-1.</em></p><p style='text-align: center;'><em>Let a sequence of Visits to specific POIs be called an n-visit where n is the number of POIs as a numeral prefix.</em></p><p style='text-align: center;'>&nbsp;</p><p><strong>Rationalizing N-MMC For Trip Prediction</strong></p><p>Why do we consider context at all when making predictions? Why not just use a 1-MMC in which our state diagram devolves into a bubble chart? While a 1-MMC does capture preference, it fails to account for the structure of the data. Remember that time determines the order of the Visits in a MobilityTrace and if we don&rsquo;t consider the order of the Visits we miss out on the hidden features that govern human mobility like the distance between POIs and POI Category.</p><p>Consider the bi-visit &ldquo;McDonalds SplashWorld&rdquo; This would be very rare because most people do not like to eat immediately before going swimming. This rule would be part of what I call a &ldquo;mobility grammar.&rdquo; Another rule that most people follow is that they visit POIs near each other or on their trajectory of travel. As mobility grammars are far from universal, we do not write these rules but capture them empirically through building N-MMCs.</p><p>The most important characteristic of an N-MMC is that it obey the Markov Property allowing us to consider only a limited amount of context. If we consider too much context, we will have a data sparsity problem in which the training examples will be insufficient to make accurate predictions. Conversely, if we use to little context, then we may not capture essential relationships. While we know in the case of general mobility modeling, it is best to go with a 2-MMC model it is unclear how much context is best when it comes to trip prediction. My intuition was that a similar amount of context would be appropriate; however, this was only a hypothesis which needed validation to ensure the correct parameter be selected. We validate this hypothesis in the next section through empirical means.</p><p>&nbsp;</p><p><strong>Calculating Conditional Probability</strong></p><p>Now I want to explain how we can calculate the probabilities of transition for a given N-MMC. We are going to approach this calculation more formally so bear with me.</p><p>&nbsp;</p><p style='text-align: center;'><em>Let the occurrence of an n-Visit as defined above be analogous to an event in the probability space </em></p><p style='text-align: center; line-height: normal;'><em>Markov Assumption: Let P(S<sub>k</sub>|S<sub>1</sub>S<sub>2</sub>S<sub>3</sub>&hellip;S<sub>k-1</sub>) approx equal </em><em>P(S<sub>k</sub>|S<sub>k+1-n</sub>&hellip;S<sub>k-1</sub>) &nbsp;for N &gt; 1</em></p><p style='text-align: center; line-height: normal;'>&nbsp;</p><p style='line-height: normal;'>Consider a MobilityTrace containing the sequence of Visits: &ldquo;McDonald&rsquo;s Walmart Starbucks FedEx.&rdquo; In a 3-MMC model the probability of the sequence &ldquo;FedEx&rdquo; is only dependent on the sequence &ldquo;Walmart Starbucks&rdquo; appearing before it. Thus, we ignore McDonald's and can write the conditional probability as:</p><p style='line-height: normal;'>&nbsp;</p><p style='text-align: center; line-height: normal;'><em>P(FedEx | Walmart Starbucks) = P(Walmart Starbucks FedEx)/P(Walmart Starbucks)</em></p><p style='text-align: center; line-height: normal;'>&nbsp;</p><p style='line-height: normal;'>To calculate this probability, our model must contain counts of n-visits. To find the conditional probability, we will need to find the probability of the bi-visit &ldquo;Walmart Starbucks&rdquo; and the tri-visit &ldquo;Walmart Starbucks FedEx.&rdquo; We calculate these probabilities by counting how often they occur in the training data. Note that in a 3-MMC we must only actually store the tri-visits as we can derive bi-visit counts of &ldquo;Walmart Starbucks&rdquo; just by adding up all tri-visits that contain that bi-visit.</p><p style='line-height: normal;'>&nbsp;</p><p style='text-align: center; line-height: normal;'><em>Let C(x) be a count function for a specific n-visit </em><br /><em>Let T be the number of n-visits in the MobilityTrace with length indicated by the subscript</em><br /><em>P (Walmart Starbucks FedEx) = C(Walmart Starbucks FedEx)/T<sub>3</sub></em><br /><em>P(Walmart Starbucks) = C(Walmart Starbucks)/T<sub>2</sub></em><br /><em>* Together then: P(FedEx | Walmart Starbucks) = C(Walmart Starbucks FedEx)/N<sub>3</sub> / C(Walmart Starbucks)/N<sub>2</sub></em></p><p style='text-align: center; line-height: normal;'>&nbsp;</p><p style='line-height: normal;'>We can simplify * because N<sub>2</sub> will always be one more than N<sub>3</sub>. For large counts this tiny difference will not significantly impact the probability- we almost have symmetry. Let us use a concrete example to prove this.</p>&nbsp;<p style='text-align: center; line-height: normal;'><em>MobilityTrace: &ldquo;Walmart Starbucks is Walmart Starbucks and when Walmart Starbucks FedEx he howls&rdquo;</em><br /><em>C(Walmart Starbucks FedEx) = 1</em><br /><em>C(Walmart Starbucks) = 3</em><br /><em>N<sub>3</sub> = 10 and thus N<sub>2</sub> must equal 11</em><br /><em>Proof: 1/10 / 3/11 approximately equal to 1/3</em></p><p style='text-align: center; line-height: normal;'>&nbsp;</p><p style='line-height: normal;'><strong>Making Predictions</strong></p><p style='line-height: normal;'>We have seen thus far how we find the probability for a specified state given context but what if we have context and need to make a prediction? Consider the context &ldquo;Walmart Starbucks,&rdquo; how do we decide which state comes next? We use our Ngram model to build a probability distribution using the counts of all tri-visits which start with the bi-visit 'Walmart Starbucks.'</p><p style='line-height: normal;'>&nbsp;</p><p style='text-align: center; line-height: normal;'><em>Walmart Starbucks Costco =2 </em><br /><em>Walmart Starbucks Shell = 3 </em><br /><em>Walmart Starbucks Lowe&rsquo;s &nbsp;=4 </em><br /><em>This yield a distribution with 2/9, 3/9, 2/9.</em></p><p style='text-align: center; line-height: normal;'>&nbsp;</p><p style='line-height: normal;'>Looking at this distribution, we would choose &ldquo;Lowe&rsquo;s&rdquo; as the most likely next state because it has the highest probability. Utilizing an N-MMC in such a way is called Maximum Likelihood Estimation (MLE).</p><p style='line-height: normal;'>By now we should have a solid understanding of the N-MMC- a powerful tool for next place prediction. In the following article, we will tailor this tool for trip prediction.</p>"
                }]
            }];







    }
});
