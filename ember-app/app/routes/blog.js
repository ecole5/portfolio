import Route from '@ember/routing/route';

export default Route.extend({

    model() {

        return [


            {
                title: "GeoPredict",
                body: "<p><strong>Introduction&nbsp; </strong></p> <p>This series of blog posts details the development of a novel machine learning solution called GeoPredict. I designed GeoPredict with the intent of leveraging large-scale mobility data in a commercially viable manner. I conducted this effort on behalf of an industry partner while working in the laboratory of the Western Engineering Dean of Research, in my capacity as a Summer Research Associate.</p> <p>The first step towards the development of GeoPredict was gaining a solid understanding of the problem domain-the data. The data provided by my industry partner was not just raw mobility data but a richer set of contextual and semantic information. Let us try to unpack the structure of this data.</p> <p>&nbsp;</p> <p><strong>Understanding the Data</strong></p> <p>We call a set of data a MobilityTrace and a discrete data point a Visit. My partner collected the provided MobilityTrace from their user base (millions of users) over three months.</p><div class='text-center'> <img src='/img/projects/GeoPredict/1a.png' class='img-fluid py-5' style='max-width:70%;'></div>&nbsp; <p>A Visit is created when a Source captures a new Coordinate. A Coordinate contains the position of the Source (longitude and latitude) and the precision of this measurement.</p> <p>Precision is the attribute of a Coordinate accounting for the uncertainty in the position calculation. It is the radius of the circle surrounding the given position. Each Source uses different methods for determining position. These include but are not limited to WiFi triangulation, cellular triangulation, and GPS. Each of these methods has different ranges of baseline precision that vary due to environmental factors.</p> <p>A Source itself is the specific platform upon which my partner&rsquo;s application runs. Sources include web browsers, mobile operating systems, and mobile browsers.&nbsp; As people tend to keep their cell phones on their person Visits collected from mobile Sources are vital because they serve as a better proxy for human mobility than static Sources such as a desktop web browser.</p> <p>A userID is an attribute assigned to each Visit to enable analysis on a per-user basis. A user could have multiple ids if they are not logged in across Sources. For privacy, my partner opted not to tie each Visit to a complete UserProfile but only the numeric userID.&nbsp;</p> <p>The most significant preprocessing step on the provided MobilityTrace was tagging each Visit with a commercial POI (point of interest). Each POI is an individual location belonging to a POIGroup. For example, Starbucks is a POIGroup, but the Starbucks with the address 1 Bayside Drive is a POI.</p> <p>POI tagging was accomplished using a commercial database licensed by my partner. My partner removed all Visits that could not be tagged.&nbsp; This purge is significant in that any value to be had from the MobilityTrace would relate to shopping trips as opposed to general mobility.</p> <p><strong>&nbsp;</strong></p> <p><strong>Unsupervised Approach</strong></p> <p>Once I fully understood the nature of the data, I turned my attention to refining my goal: what type of value could my solution deliver? My first intuition was to see if I could uncover any hidden structural value with an unsupervised learning approach. I started with just one feature, using the distance between the Coordinates to cluster the Visits with k-means. Upon inspection of the resulting clusters, I realized that I had stumbled upon a method of discovering POIs. At a low resolution the group labels represented entire geographic regions (cities and towns), but at a higher resolution, the labels signified POIs.</p> <p>By utilizing a third-party database, my partner was already able to label these clusters. I wondered, however, what would happen as new POIs emerged. Would my partner depend on their data provider to provide timely updates? Was there a way I could build a system to discover and label new POIs to break this dependence? I started brainstormed some potential solutions.</p> <p>The first approach that came to mind was to match the cluster coordinates to an address on a city map and then build a web crawler to find mentions of the address on social media with the intention of extracting a POI name. Another potential solution would be to collect photos geotagged with cluster&rsquo;s coordinates and use computer vision techniques to obtain the names of the storefronts in these pictures. I reckoned Google might use a similar approach with the vast amount of Street View imagery.&nbsp; Although both these techniques were of interest to me from an academic standpoint, they were outside my operating parameters in that in that they would require additional data (city maps, user photos) and thus not be commercially viable to my partner.</p> <p>I continued by trying to cluster a higher dimensional set of features considering time, distance and category. I used principal component analysis to produce a unified distance metric for input into k-means. Upon inspection of the results, I was unable to draw any useful conclusions. I decided it was time to move on.</p> <p><strong>&nbsp;</strong></p> <p><strong>Supervised Approach: Recommender Systems</strong></p> <p>Frustrated with my lack of progress I resolved to binge-watch an entire season of Stranger Things on Netflix. Upon completion of the season, I was left impressed. Not just by the wonderfully nostalgic world of Hawkins but with Netflix's suggestion for what I might watch next.&nbsp; Netflix aptly combined the context of Stranger Things with my viewing habits to come up with a strong set of candidates. It was the idea of context that intrigued me. If I liked Stranger Things, then I would probably like Aliens too. Could I use the context of a recent Visit to predict the next one?</p> <p>What makes the ability to predict a user's next Visit valuable? Suppose a user, Alice, is currently at a Goodlife Fitness and we have a recommender system predict that she will go to McDonald's next. Other business in the Food &amp; Beverage space could participate in a real-time bid for the ability to market to Alice right before she makes her decision. The confidence of the recommendation could help the bidders tune their bids.&nbsp; A second potential application is an emergency alert system. Let us say that the police just evacuated the McDonalds Alice was likely to visit; an alert could be sent to Alice warning her to steer clear. The applications of these recommendations go on and on- the value is obvious. Additionally, the system seemed feasible given the MobilityTrace I was working with and my time constraints. Thus it was clear- my goal would be to build a near real-time recommender system for shopping trip forecasting.</p>"
                
            },
               
            {
                title: "SparkPlug",
                body: "<p><strong>Introduction</strong></p><p>The world is undergoing a big data revolution- a new age gold rush. Never in the history of human civilization has there been such an abundance of data. Remarkably, however, most of this data goes to waste. The situation many organizations find themselves in today is analogous to sitting on vast reserves of gold-rich ore but not knowing how to extract it.&nbsp; The difficulty with big data lies in both variety and scale. Firstly, there is no perfect cookie-cutter solution: every data set requires a unique methodology that accounts for its domain-specific features. Secondly, real-world big data implementations must scale across compute clusters to handle massive volume and velocity thus adding significant complexity.</p><p>Having just finished developing GeoPredict, a recommender for shopping trip prediction, I was familiar with one side of the big data coin. GeoPredict addressed the specificity of the mobility domain, but it was not designed to scale. If GeoPredict were to be of any real value to my partner, it would need to be capable of making near real-time predictions for millions of users simultaneously. To accomplish this in an economically viable way, I knew I needed to enable GeoPredict to scale horizontally across commodity hardware. Also, I needed a way to integrate my solution with my partner&rsquo;s system quickly.&nbsp;&nbsp; I launched this newfound effort as my CAPSTONE project while maintaining the stakeholders from the original GeoPredict effort.</p><p>&nbsp;</p><p><strong>Scaling GeoPredict</strong></p><p>The key to horizontal scale is to distribute the computation across a cluster.&nbsp; Luckily many big data engines exist that help abstract away the enormous complexity of designing these parallel applications. After researching my options, I landed on the Spark engine as it offers better performance than modern Hadoop MapReduce and has broad adoption in industry. Given that I already knew SQL I implemented GeoPredict as a series of database operations using the SparkSQL library with the PySpark API.</p><p>To achieve interoperability, I decided that the system should be completely decoupled from my partner&rsquo;s. Thus I decided to implement a microservice architecture in which communication between the two systems occurred over a RESTful API. I went ahead and encapsulated different functions of GeoPredict into routes of the API and designed a way to submit each route to the Spark cluster for on-demand computation.</p><p>&nbsp;</p><p><strong>Towards A Framework</strong></p><p>When I had finished implementing the architecture, I needed a way to make it easy for my partner to deploy the system. I found Spark cluster configuration to be quite tedious, so I decided to containerize the entire architecture and the Spark cluster itself with Docker.</p><p>After finishing my work and demonstrating the system to my supervisor and partner I came to understand that the real value in what I had built was not a scalable version of GeoPredict but an architecture and deployment strategy which could form the basis of a framework I call <strong>SparkPlug</strong>.</p><p>To realize the vision of a framework, as opposed to a specific implementation I developed the SparkPlug CLI. The CLI provides a user-friendly way to build out the SparkPlug architecture and automates the deployment and configuration of the containerized infrastructure.</p><p>By providing an interoperable architecture and a containerized deployment strategy SparkPlug makes it possible to bring data solutions to market rapidly. SparkPlug enables data scientist with limited programming experience to create commercial solutions to big data problems.</p><p>In the next few blog offices, we will dive into the details of the three central components of SparkPlug: the Microservice Architecture, the Deployment Strategy and SparkPlug CLI.</p>"

            }
            


        ];



    }
});
