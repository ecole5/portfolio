import Route from "@ember/routing/route";

export default Route.extend({
  model() {
    return [
      {
        title: "GeoPredict",
        pic: ["pandas", "jupyter", "python"],
        body:
          "<p class='card-text'>GeoPredict is a novel recommender system for shopping trip prediction. The system combines shopping history with timely contextual clues to deliver probabilistic forecasts of future movement to commercial locations. GeoPredict is built using specially trained Markov Chains and borrows techniques from many disciplines across the AI landscape.</p><p class='card-text'>GeoPredict was initially developed in Python using the Pandas library and has since been redeveloped using the SparkPlug framework to handle big data and enable interoperability.</p><p class='card-text'>At the request of my partner, the source code remains proprietary. However, I have been cleared to provide a detailed technical rundown of my development process which you may find below.</p>",

        posts: [
          {
            title: "Part 1: Origins & Data Exploration",
            snippet:
              "In this section we start by exploring the data and defining the problem domain.",
            body:
              "<p><strong>Introduction&nbsp; </strong></p> <p>This series details the development of a novel machine learning solution called GeoPredict. I designed GeoPredict with the intent of leveraging large-scale mobility data in a commercially viable manner. I conducted this effort on behalf of an industry partner while working in the laboratory of the Western Engineering Dean of Research, in my capacity as a Summer Research Associate.</p> <p>The first step towards the development of GeoPredict was gaining a solid understanding of the problem domain-the data. The data provided by my industry partner was not just raw mobility data but a richer set of contextual and semantic information. Let us try to unpack the structure of this data.</p> <p>&nbsp;</p> <p><strong>Understanding the Data</strong></p> <p>We call a set of data a MobilityTrace and a discrete data point a Visit. My partner collected the provided MobilityTrace from their user base (millions of users) over three months.</p><div class='text-center'> <img src='/img/projects/GeoPredict/1a.png' class='img-fluid py-5' style='max-width:70%;'></div>&nbsp; <p>A Visit is created when a Source captures a new Coordinate. A Coordinate contains the position of the Source (longitude and latitude) and the precision of this measurement.</p> <p>Precision is the attribute of a Coordinate accounting for the uncertainty in the position calculation. It is the radius of the circle surrounding the given position. Each Source uses different methods for determining position. These include but are not limited to WiFi triangulation, cellular triangulation, and GPS. Each of these methods has different ranges of baseline precision that vary due to environmental factors.</p> <p>A Source itself is the specific platform upon which my partner&rsquo;s application runs. Sources include web browsers, mobile operating systems, and mobile browsers.&nbsp; As people tend to keep their cell phones on their person Visits collected from mobile Sources are vital because they serve as a better proxy for human mobility than static Sources such as a desktop web browser.</p> <p>A userID is an attribute assigned to each Visit to enable analysis on a per-user basis. A user could have multiple ids if they are not logged in across Sources. For privacy, my partner opted not to tie each Visit to a complete UserProfile but only the numeric userID.&nbsp;</p> <p>The most significant preprocessing step on the provided MobilityTrace was tagging each Visit with a commercial POI (point of interest). Each POI is an individual location belonging to a POIGroup. For example, Starbucks is a POIGroup, but the Starbucks with the address 1 Bayside Drive is a POI.</p> <p>POI tagging was accomplished using a commercial database licensed by my partner. My partner removed all Visits that could not be tagged.&nbsp; This purge is significant in that any value to be had from the MobilityTrace would relate to shopping trips as opposed to general mobility.</p> <p><strong>&nbsp;</strong></p> <p><strong>Unsupervised Approach</strong></p> <p>Once I fully understood the nature of the data, I turned my attention to refining my goal: what type of value could my solution deliver? My first intuition was to see if I could uncover any hidden structural value with an unsupervised learning approach. I started with just one feature, using the distance between the Coordinates to cluster the Visits with k-means. Upon inspection of the resulting clusters, I realized that I had stumbled upon a method of discovering POIs. At a low resolution the group labels represented entire geographic regions (cities and towns), but at a higher resolution, the labels signified POIs.</p> <p>By utilizing a third-party database, my partner was already able to label these clusters. I wondered, however, what would happen as new POIs emerged. Would my partner depend on their data provider to provide timely updates? Was there a way I could build a system to discover and label new POIs to break this dependence? I started brainstormed some potential solutions.</p> <p>The first approach that came to mind was to match the cluster coordinates to an address on a city map and then build a web crawler to find mentions of the address on social media with the intention of extracting a POI name. Another potential solution would be to collect photos geotagged with cluster&rsquo;s coordinates and use computer vision techniques to obtain the names of the storefronts in these pictures. I reckoned Google might use a similar approach with the vast amount of Street View imagery.&nbsp; Although both these techniques were of interest to me from an academic standpoint, they were outside my operating parameters in that in that they would require additional data (city maps, user photos) and thus not be commercially viable to my partner.</p> <p>I continued by trying to cluster a higher dimensional set of features considering time, distance and category. I used principal component analysis to produce a unified distance metric for input into k-means. Upon inspection of the results, I was unable to draw any useful conclusions. I decided it was time to move on.</p> <p><strong>&nbsp;</strong></p> <p><strong>Supervised Approach: Recommender Systems</strong></p> <p>Frustrated with my lack of progress I resolved to binge-watch an entire season of Stranger Things on Netflix. Upon completion of the season, I was left impressed. Not just by the wonderfully nostalgic world of Hawkins but with Netflix's suggestion for what I might watch next.&nbsp; Netflix aptly combined the context of Stranger Things with my viewing habits to come up with a strong set of candidates. It was the idea of context that intrigued me. If I liked Stranger Things, then I would probably like Aliens too. Could I use the context of a recent Visit to predict the next one?</p> <p>What makes the ability to predict a user's next Visit valuable? Suppose a user, Alice, is currently at a Goodlife Fitness and we have a recommender system predict that she will go to McDonald's next. Other business in the Food &amp; Beverage space could participate in a real-time bid for the ability to market to Alice right before she makes her decision. The confidence of the recommendation could help the bidders tune their bids.&nbsp; A second potential application is an emergency alert system. Let us say that the police just evacuated the McDonalds Alice was likely to visit; an alert could be sent to Alice warning her to steer clear. The applications of these recommendations go on and on- the value is obvious. Additionally, the system seemed feasible given the MobilityTrace I was working with and my time constraints. Thus it was clear- my goal would be to build a near real-time recommender system for shopping trip forecasting.</p>",
          },
          {
            title: "Part 2: Mobility Markov Chains (MMCs)",
            snippet:
              "Now we explore the Mobility Markov Chain forming the basis of our model.",
            body:
              "<p><strong>Introduction</strong></p><p>Having gained a solid grasp of both the problem and solution domains it was time to move towards implementation. How do we achive effective next place prediction using MobilityTraces only containing Visits to commercial POIs? Given that trip prediction is a subset of the general mobility problem I figured it was unnecessary to reinvent the wheel, so I started researching techniques which consider complete MobilityTraces. After reviewing the literature, I landed on Markov Chains as the basis of my implementation. While there are techniques involving neural networks that are slightly more accurate, they don't pay dividends considering their complexity and required computing power. The drivers of human mobility are not so involved as to need neural layers to capture them.</p><p>&nbsp;</p><p><strong>Mobility Markov Chains</strong></p><p>A Markov Chain is a stochastic model in which the frequency of transition between states governs the probability distribution of the random variable. Many subdisciplines of A.I. use Markov Chains in some way or another, and every application has its domain-specific nomenclature. In the mobility context, we adopt the name Mobility Markov Chain (MMC). MMCs have an intuitive visualization as a state diagram as seen below.</p><div class='text-center'><img src='/img/projects/GeoPredict/2a.png' class='img-fluid py-5' style='max-width:40%;'></div><p>In this MMC the nodes signify state (a specific POI) while the labels of the directed edges represent the probability of transition from one POI to the next. The simplest way to train an MMC is to record the counts of past transitions between two states using a set of training data (MobilityTrace). At prediction time we can build a probability distribution by looking at all the edges connected to the current state.</p><p>The MMC above considers one piece of context, the current POI, to define the state of the system. However, we could consider more context in our predictions and calculate the transition probabilities not just based on a single edge put on a path. Let us define some new terminology to describe the amount of context used in the calculation of the transition probabilities. &nbsp;&nbsp;&nbsp;</p> &nbsp;<p style='text-align: center;'><em> Let an (N-MMC) be an MMC in which we calculate probabilities for paths of length N-1.</em></p><p style='text-align: center;'><em>Let a sequence of Visits to specific POIs be called an n-visit where n is the number of POIs as a numeral prefix.</em></p><p style='text-align: center;'>&nbsp;</p><p><strong>Rationalizing N-MMC For Trip Prediction</strong></p><p>Why do we consider context at all when making predictions? Why not just use a 1-MMC in which our state diagram devolves into a bubble chart? While a 1-MMC does capture preference, it fails to account for the structure of the data. Remember that time determines the order of the Visits in a MobilityTrace and if we don&rsquo;t consider the order of the Visits we miss out on the hidden features that govern human mobility like the distance between POIs and POI Category.</p><p>Consider the bi-visit &ldquo;McDonalds SplashWorld&rdquo; This would be very rare because most people do not like to eat immediately before going swimming. This rule would be part of what I call a &ldquo;mobility grammar.&rdquo; Another rule that most people follow is that they visit POIs near each other or on their trajectory of travel. As mobility grammars are far from universal, we do not write these rules but capture them empirically through building N-MMCs.</p><p>The most important characteristic of an N-MMC is that it obey the Markov Property allowing us to consider only a limited amount of context. If we consider too much context, we will have a data sparsity problem in which the training examples will be insufficient to make accurate predictions. Conversely, if we use to little context, then we may not capture essential relationships. While we know in the case of general mobility modeling, it is best to go with a 2-MMC model it is unclear how much context is best when it comes to trip prediction. My intuition was that a similar amount of context would be appropriate; however, this was only a hypothesis which needed validation to ensure the correct parameter be selected. We validate this hypothesis in the next section through empirical means.</p><p>&nbsp;</p><p><strong>Calculating Conditional Probability</strong></p><p>Now I want to explain how we can calculate the probabilities of transition for a given N-MMC. We are going to approach this calculation more formally so bear with me.</p><p>&nbsp;</p><p style='text-align: center;'><em>Let the occurrence of an n-Visit as defined above be analogous to an event in the probability space </em></p><p style='text-align: center; line-height: normal;'><em>Markov Assumption: Let P(S<sub>k</sub>|S<sub>1</sub>S<sub>2</sub>S<sub>3</sub>&hellip;S<sub>k-1</sub>) approx equal </em><em>P(S<sub>k</sub>|S<sub>k+1-n</sub>&hellip;S<sub>k-1</sub>) &nbsp;for N &gt; 1</em></p><p style='text-align: center; line-height: normal;'>&nbsp;</p><p style='line-height: normal;'>Consider a MobilityTrace containing the sequence of Visits: &ldquo;McDonald&rsquo;s Walmart Starbucks FedEx.&rdquo; In a 3-MMC model the probability of the sequence &ldquo;FedEx&rdquo; is only dependent on the sequence &ldquo;Walmart Starbucks&rdquo; appearing before it. Thus, we ignore McDonald's and can write the conditional probability as:</p><p style='line-height: normal;'>&nbsp;</p><p style='text-align: center; line-height: normal;'><em>P(FedEx | Walmart Starbucks) = P(Walmart Starbucks FedEx)/P(Walmart Starbucks)</em></p><p style='text-align: center; line-height: normal;'>&nbsp;</p><p style='line-height: normal;'>To calculate this probability, our model must contain counts of n-visits. To find the conditional probability, we will need to find the probability of the bi-visit &ldquo;Walmart Starbucks&rdquo; and the tri-visit &ldquo;Walmart Starbucks FedEx.&rdquo; We calculate these probabilities by counting how often they occur in the training data. Note that in a 3-MMC we must only actually store the tri-visits as we can derive bi-visit counts of &ldquo;Walmart Starbucks&rdquo; just by adding up all tri-visits that contain that bi-visit.</p><p style='line-height: normal;'>&nbsp;</p><p style='text-align: center; line-height: normal;'><em>Let C(x) be a count function for a specific n-visit </em><br /><em>Let T be the number of n-visits in the MobilityTrace with length indicated by the subscript</em><br /><em>P (Walmart Starbucks FedEx) = C(Walmart Starbucks FedEx)/T<sub>3</sub></em><br /><em>P(Walmart Starbucks) = C(Walmart Starbucks)/T<sub>2</sub></em><br /><em>* Together then: P(FedEx | Walmart Starbucks) = C(Walmart Starbucks FedEx)/N<sub>3</sub> / C(Walmart Starbucks)/N<sub>2</sub></em></p><p style='text-align: center; line-height: normal;'>&nbsp;</p><p style='line-height: normal;'>We can simplify * because N<sub>2</sub> will always be one more than N<sub>3</sub>. For large counts this tiny difference will not significantly impact the probability- we almost have symmetry. Let us use a concrete example to prove this.</p>&nbsp;<p style='text-align: center; line-height: normal;'><em>MobilityTrace: &ldquo;Walmart Starbucks is Walmart Starbucks and when Walmart Starbucks FedEx he howls&rdquo;</em><br /><em>C(Walmart Starbucks FedEx) = 1</em><br /><em>C(Walmart Starbucks) = 3</em><br /><em>N<sub>3</sub> = 10 and thus N<sub>2</sub> must equal 11</em><br /><em>Proof: 1/10 / 3/11 approximately equal to 1/3</em></p><p style='text-align: center; line-height: normal;'>&nbsp;</p><p style='line-height: normal;'><strong>Making Predictions</strong></p><p style='line-height: normal;'>We have seen thus far how we find the probability for a specified state given context but what if we have context and need to make a prediction? Consider the context &ldquo;Walmart Starbucks,&rdquo; how do we decide which state comes next? We use our Ngram model to build a probability distribution using the counts of all tri-visits which start with the bi-visit 'Walmart Starbucks.'</p><p style='line-height: normal;'>&nbsp;</p><p style='text-align: center; line-height: normal;'><em>Walmart Starbucks Costco =2 </em><br /><em>Walmart Starbucks Shell = 3 </em><br /><em>Walmart Starbucks Lowe&rsquo;s &nbsp;=4 </em><br /><em>This yield a distribution with 2/9, 3/9, 2/9.</em></p><p style='text-align: center; line-height: normal;'>&nbsp;</p><p style='line-height: normal;'>Looking at this distribution, we would choose &ldquo;Lowe&rsquo;s&rdquo; as the most likely next state because it has the highest probability. Utilizing an N-MMC in such a way is called Maximum Likelihood Estimation (MLE).</p><p style='line-height: normal;'>By now we should have a solid understanding of the N-MMC- a powerful tool for next place prediction. In the following article, we will tailor this tool for trip prediction.</p>",
          },
          {
            title: "Part 3: Improvements and Considerations",
            snippet:
              "Now we build better N-MMC to meet the demands of trip prediction.",
            body:
              "<p><strong>Introduction</strong></p> <p>The N-MMC we have developed thus far are excellent tools for the general problem of next place prediction. Now we must focus on adapting the model to accommodate the complications of trip prediction.</p> <p>&nbsp;</p> <p><strong>Trip Separation</strong></p> <p>The structure of the data poses the first complication. So far we have assumed that all Visits in a MobilityTrace are sequential, but in reality, they are not because Visits to private locations have been removed. Non-sequential Visits results in false inferences when training our N-MMC. To illustrate this point consider training a 2-MMC using the following MobilityTrace:</p> <p>&nbsp;</p> <p style='text-align: center;'><em>Bed Bath Beyond -&gt; 7:00</em></p> <p style='text-align: center;'><em>McDonald's -&gt; 7:30</em></p> <p style='text-align: center;'><em>SplashWorld -&gt; 20:00</em></p> <p>&nbsp;</p> <p>There are two bi-visits in this MobilityTrace: &ldquo;Bed Bath Beyond McDonald's&rdquo; and &ldquo;McDonald's SplashWorld&rdquo; each with a single occurrence. Thus, if given the context McDonald's we would predict SplashWorld as the next POI. Looking at the timestamps, we notice that McDonalds and SplashWorld are many hours apart. It is reasonable to assume that the user didn&rsquo;t just stay at McDonald's till 20:00 and likely visited other personal POIs in between McDonald's and SplashWorld that were removed. These Visits are not sequential, yet we treat them as if they are. If we don&rsquo;t rectify this issue, we might make the false inference that people tend to eat right before they swim which for many people is not the case.</p> <p>Another way to think about this MobilityTrace is that it is composed of two separate trips.&nbsp; The first trip in the morning and the second at night. To avoid false inferences, we must make sure that all visits in an n-visit are from the same trip. Luckily it is relatively straightforward to create these groupings by merely looking at the amount of time between two Visits.</p> <p>&nbsp;</p> <p style='text-align: center;'><em>For two &ldquo;sequential&rdquo; Visits in a MobilityTrace let the first Visit occur at time T<sub>a</sub> and the second Visit with time T<sub>b</sub>.</em></p> <p style='text-align: center;'><em>Let T<sub>e</sub> be a hyperparameter defining a time threshold between two trips</em></p> <p style='text-align: center;'><em>If |T<sub>b</sub>-T<sub>a</sub>| &gt; T<sub>e</sub> punctuate MobilityTrace with End of Trip (EoT) after T<sub>a</sub></em></p> <p>&nbsp;</p> <p>How do we figure out the correct hypermeter T<sub>e</sub> to prevent false inferences? The ideal solution would be to optimize T<sub>e</sub> using a duplicate MobilityTrace containing EoT markers. Without this labeled MobilityTrace we don&rsquo;t have ground truth, and the best we can do is make an educated guess- somewhere around three to four hours should be ideal.</p> <p>&nbsp;</p> <p><strong>Selecting the Right (N)-MMC</strong></p> <p>Now that we have split our MobilityTrace into trips, we move on to choosing the correct amount of context. The MobilityTrace provided by my partner was captured over three months, and the number of Visits recorded for each user varied significantly meaning that the correct N might differ depending on the user.</p> <p>My intuition was that the N value appropriate for trip prediction would be in the neighborhood of the N value for the general mobility problem (N=2).&nbsp; The big difference between general mobility and trip prediction is that long n-visits are less frequent because they get cut off when trips end, thus a smaller N value might be better because n-visits of a high order would mean many unseen contexts.</p> <p>I moved forward and conducted an empirical analysis, comparing N=1 to N=5 MMC for every user.&nbsp; The data showed that for most users the 2-MMC model was the best. For the most active users (the top 10%), however, a 3-MMC was better. If we had six months of data would this change anything? The answer is not clear because while we would have more Visits, perhaps the prior Visits would be outdated. It may be the case that 3-MMC only works well for users that go on long shopping trips.</p> <p>&nbsp;</p> <p><strong>Smoothing</strong></p> <p>Maximum likelihood estimation is an adequate technique if all we care about it trying to make the best guess possible with the data we have. A great deal of value, however, comes from being able to present a probability distribution for every prediction so that the data is more actionable based on the tolerances of error for the desired application. For example, if GeoPredict is being used to send emergency alert messages warning people to steer clear of a specific POI, any POI in the distribution with 10% likelihood or higher might be considered actionable.&nbsp; If GeoPredict is used for targeted advertising, then perhaps you might only want to push ads when there is a &gt;60% chance of the next Visit being to a particular POI.</p> <p>If we care about the accuracy of the distribution, then we must adjust for the unseen events in the probability space (the unseen n-visits). As N increases the number of possible paths we can take through our MMC explodes by #POI<sup>n</sup>. As these paths (n-visits) increase the data set becomes sparer. We can define the total number of possible POIs as the number of POIs in a geographic region available in the Google Maps database.&nbsp; There are several methods for smoothing our distributions to resolve the sparsity problem, but perhaps the best is an adapted version of Good-Turing frequency estimation which works by borrowing some of the probability space from the seen events and allocating it to the unseen events. This technique is commonly employed in NLP for similar purposes.</p> <p>&nbsp;</p> <p><strong>Multiple Models</strong></p> <p>A 3-MMC inherently contains a 2-MMC and 1-MMC inside of it thus we only need to store a 3-MMC (tri-visits) and can derive the counts of bi-visits and uni-visits. However, this process is computationally expensive, and we are looking to maximize speed to achieve near real-time predictions it is best to make a time-space tradeoff by generating hash tables to store the count of all n-visits from n=1 to the desired N. While to make predictions with a 3-MMC we only actually need tri-visits and bi-visits it is prudent to keep uni-visits because it allows us the flexibility to use a lower order N model if we don&rsquo;t have enough context so that we can always make some educated prediction.</p> <p>&nbsp;</p> <p><strong>Expanded Context (Day of The Week)</strong></p> <p>So far we have constructed a model in which the state is represented only by physical and semantic location. What about time? Can we use temporal context to aid in predictions? To realize this expanded context I built 7 N-MMC as opposed to one in which each N-MMC was trained using trips from a particular day of the week. The idea here is that many commercial offers and promotions occur on a specific day of the week and that may be a determent of consumer shopping habits.</p> <p>I tested my day of the week N-MMCs by considering the day of the week at prediction time and making the prediction only using the associated N-MMC. Unfourtantly the results were worse than just using a single N-MMC. There just wasn&rsquo;t enough data to make this approach work because so many n-visits were unseen and thus the system kept reverting down to uni-MMC to make predictions. Perhaps with more Visits (6 month collection period), this would be worth another shot.</p> <p>&nbsp;</p> <p><strong>Training/Operation Phase Notes</strong></p> <p>During training, we can think of each trip as a MobilityTrace which we train separately on the same table of counts.&nbsp; At the start of the trip we have no context, so we can only count uni-visits and then as we get more context we count bigger n-visits up to the (N) parameter of the selected model (N=2 or N=3).</p> <p>When making predictions (operation phase) we receive Visits on a continual basis as they occur. We cannot know if the next Visit we will receive will be on a different trip thus we always make a prediction as soon as a Visit is received. We do need to consider the case where N &gt; 2 as we can apply of threshold Te between the context to ensure it is from the same trip. Remember that if the time gap is over the threshold and we have to through away the first piece of context we can revert to a 2-MMC model.</p> <p>During prediction phase, we also must make sure that the timestamp on the most recent Visit we receive is close to the current system time.&nbsp; If there is too much of a delay in receiving the Visit, it is likely the user always transitioned.</p> <p>All the visits we capture during the operation phase can be saved so we can update the model's counts in the future.</p> <p>&nbsp;</p> <p><strong>Conclusion</strong></p> <p>So there we have it- a novel recommender system to address the problem of shopping trip prediction.&nbsp; Of course, as with any data solution, there is lots of room for improvement particularly in the area of metric development and testing. Overall, however, GeoPredict performs similarly to recommender systems for general mobility- which is an excellent result. To see how we scale GeoPredict to handle big data read my series on SparkPlug.</p>",
          },
        ],
      },
      {
        title: "SparkPlug",
        pic: ["docker", "spark", "swarm", "unix", "ec2", "hadoop"],
        body:
          "<p class='card-text'>SparkPlug is a framework for the rapid development and deployment of distributed Spark applications. By providing an interoperable architecture and a containerized deployment strategy SparkPlug makes it possible to rapidly bring data solutions to market without a great deal of programming expertise.</p><p class='card-text'>SparkPlug applications are built and deployed using an easy to use command line interface that provides blueprinting, service management and configuration. SparkPlug applications are implemented with a Microservice Architecture in which big data tasks running on the cluster are exposed via a RESTful API. Through the containerization of the entire architecture (including Spark and Hadoop clusters), SparkPlug enables instant out go the box deployment and easy scaling.</p><p class='card-text'>While the project repository is not available due to an intellectual property agreement further information about SparkPlug and its development can be found below.</p>",
        posts: [
          {
            title: "Part 1: Where do frameworks come from?",
            snippet:
              "We start by seeing how a specific solution to scale GeoPredict became a framework.",
            body:
              "<p><strong>Introduction</strong></p><p>The world is undergoing a big data revolution- a new age gold rush. Never in the history of human civilization has there been such an abundance of data. Remarkably, however, most of this data goes to waste. The situation many organizations find themselves in today is analogous to sitting on vast reserves of gold-rich ore but not knowing how to extract it.&nbsp; The difficulty with big data lies in both variety and scale. Firstly, there is no perfect cookie-cutter solution: every data set requires a unique methodology that accounts for its domain-specific features. Secondly, real-world big data implementations must scale across compute clusters to handle massive volume and velocity thus adding significant complexity.</p><p>Having just finished developing GeoPredict, a recommender for shopping trip prediction, I was familiar with one side of the big data coin. GeoPredict addressed the specificity of the mobility domain, but it was not designed to scale. If GeoPredict were to be of any real value to my partner, it would need to be capable of making near real-time predictions for millions of users simultaneously. To accomplish this in an economically viable way, I knew I needed to enable GeoPredict to scale horizontally across commodity hardware. Also, I needed a way to integrate my solution with my partner&rsquo;s system quickly.&nbsp;&nbsp; I launched this newfound effort as my CAPSTONE project while maintaining the stakeholders from the original GeoPredict effort.</p><p>&nbsp;</p><p><strong>Scaling GeoPredict</strong></p><p>The key to horizontal scale is to distribute the computation across a cluster.&nbsp; Luckily many big data engines exist that help abstract away the enormous complexity of designing these parallel applications. After researching my options, I landed on the Spark engine as it offers better performance than modern Hadoop MapReduce and has broad adoption in industry. Given that I already knew SQL I implemented GeoPredict as a series of database operations using the SparkSQL library with the PySpark API.</p><p>To achieve interoperability, I decided that the system should be completely decoupled from my partner&rsquo;s. Thus I decided to implement a microservice architecture in which communication between the two systems occurred over a RESTful API. I went ahead and encapsulated different functions of GeoPredict into routes of the API and designed a way to submit each route to the Spark cluster for on-demand computation.</p><p>&nbsp;</p><p><strong>Towards A Framework</strong></p><p>When I had finished implementing the architecture, I needed a way to make it easy for my partner to deploy the system. I found Spark cluster configuration to be quite tedious, so I decided to containerize the entire architecture and the Spark cluster itself with Docker.</p><p>After finishing my work and demonstrating the system to my supervisor and partner I came to understand that the real value in what I had built was not a scalable version of GeoPredict but an architecture and deployment strategy which could form the basis of a framework I call <strong>SparkPlug</strong>.</p><p>To realize the vision of a framework, as opposed to a specific implementation I developed the SparkPlug CLI. The CLI provides a user-friendly way to build out the SparkPlug architecture and automates the deployment and configuration of the containerized infrastructure.</p><p>By providing an interoperable architecture and a containerized deployment strategy SparkPlug makes it possible to bring data solutions to market rapidly. SparkPlug enables data scientist with limited programming experience to create commercial solutions to big data problems.</p><p>In the next few blog offices, we will dive into the details of the three central components of SparkPlug: the Microservice Architecture, the Deployment Strategy and SparkPlug CLI.</p>",
          },
        ],
      },
      {
        title: "Unity Minigame Portal",
        pic: ["unity", "csharp", "lamp"],
        body:
          "<p class='card-text'>In this project, I developed a series of mini-games in Unity 3D and built a customizable portal to access them. Undertaken as part of a design course, the development of this application was meant to be a learning experience. It turns out that building game objects are an excellent way to understand object-oriented programming and gain practical exposure to composition, inheritance, and polymorphic reuse. Additionally, I had the opportunity to implement design patterns to optimize/standardize the application architecture.</p><p class='card-text'>Given that this project was a learning experience I decided to take things a step further by extending the requirements and exploring the LAMP stack to implement a web service to achieve cloud capabilities throughout the application.</p><p class='card-text'>The application was designed to be a standalone application for computers and mobile phones. However, I have rebuilt the application using WebGL for use in your browser. As the application was not optimized for WebGL, please give it a minute or two to load.</p><p class='card-text'>To make an account login to the administrator page using the credentials below and select create user:</p><p class='card-text'>Username: admin</p><p class='card-text'>Password: admin</p>",
        slide: ["1", "2", "3", "4", "5", "6", "7"],
        link: "/ugames/play.html",
      },
    ];
  },
});
